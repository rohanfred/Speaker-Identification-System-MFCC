# -*- coding: utf-8 -*-
"""SpeakerIdentificationProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16gqIuFDEQDdukq6dQGIlBRq-NvsO3vzK
"""

!git clone https://github.com/elitecoder-0011/Speaker-Identification-System-MFCC.git

# Installing all necessary libraries
!pip install librosa
!pip install pydub
!pip install flask
!pip install pyngrok
!pip install flask pyngrok librosa numpy pandas joblib scikit-learn

import os
import librosa
import numpy as np
import pandas as pd
from pydub import AudioSegment
import warnings
import seaborn as sns
import matplotlib.pyplot as plt
import joblib
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from flask import Flask, request, render_template_string
from pyngrok import ngrok
from werkzeug.utils import secure_filename
import nest_asyncio

# Unzipping and extracting the contents of dataset file.

!unzip /content/archive.zip -d /content/extracted_archive

# Function to Convert m4a files to .wav format
def convert_to_wav(input_path, output_path):
    try:
        audio = AudioSegment.from_file(input_path, format="m4a")
        audio.export(output_path, format="wav")
        return output_path
    except Exception as e:
        print(f"Error converting audio: {e}")
        return None

# Function to compute mfcc features
def calculate_mfcc(file_path, n_mfcc=13, delta_width=3):
    try:
        audio, sample_rate = librosa.load(file_path, sr=None) # Loading the audio file
        mfcc = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc) # Computing MFCC features

        if mfcc.shape[1] < delta_width: # Ensuring the number of frames is sufficient for delta computation
            print(f"Skipping {file_path}: Too few frames ({mfcc.shape[1]} < {delta_width})")
            return None

        delta_mfcc = librosa.feature.delta(mfcc, width=delta_width) # Computing first derivative of MFCCs
        delta2_mfcc = librosa.feature.delta(mfcc, order=2, width=delta_width) # Computing second derivative of MFCCs
        # Concatenating all features: MFCC + first derivative + second derivative
        combined_mfcc = np.hstack((np.mean(mfcc, axis=1),np.mean(delta_mfcc, axis=1),np.mean(delta2_mfcc, axis=1)))
        return combined_mfcc

    except Exception as e:
        print(f"Error calculating MFCCs for {file_path}: {e}")
        return None

# Function for Preprocesssing Audio files
def preprocess_audio_files(input_dir, output_csv, n_mfcc=13, delta_width=3):
    data = []  # Initializing a list to store all processed data

    for file_name in os.listdir(input_dir): # Iterating over all audio files in the input directory
        if file_name.endswith(('.wav', '.m4a')):
            try: # Extract speaker name from the filename (e.g., '0_yweweler_39.wav' -> 'yweweler')
                speaker_name = file_name.split('_')[1]
            except IndexError:
                print(f"Invalid file format: {file_name}")
                continue
            if speaker_name in metadata: # Matching the speaker's metadata
                speaker_meta = metadata[speaker_name]
            else:
                print(f"Speaker {speaker_name} not found in metadata. Skipping...")
                continue
            file_path = os.path.join(input_dir, file_name)  # Full path to the audio file

            if file_name.endswith('.m4a'): # Converting m4a to wav
                converted_path = file_path.replace('.m4a', '.wav')
                file_path = convert_to_wav(file_path, converted_path)
                if not file_path:
                    continue

            mfcc_features = calculate_mfcc(file_path, n_mfcc=n_mfcc, delta_width=delta_width)  # Extracting MFCC features
            if mfcc_features is None:
                continue
            # Combining metadata and features
            record = {'speaker': speaker_name,'gender': speaker_meta['gender'],'accent': speaker_meta['accent'],'language': speaker_meta['language']}
            for i, coeff in enumerate(mfcc_features): # Adding MFCC features as individual columns
                record[f'mfcc_{i+1}'] = coeff
            data.append(record) # Appending to the data list

    # Creating a DataFrame and saving it as a CSV
    df = pd.DataFrame(data)
    df.to_csv(output_csv, index=False)
    print(f"Processed data saved to {output_csv}")

# Suppress warnings
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

# Metadata about the speakers
metadata = {'jackson': {'gender': 'male', 'accent': 'USA/neutral', 'language': 'english'},
    'nicolas': {'gender': 'male', 'accent': 'BEL/French', 'language': 'english'},
    'theo': {'gender': 'male', 'accent': 'USA/neutral', 'language': 'english'},
    'yweweler': {'gender': 'male', 'accent': 'DEU/German', 'language': 'english'},
    'george': {'gender': 'male', 'accent': 'GRC/Greek', 'language': 'english'},
    'lucas': {'gender': 'male', 'accent': 'DEU/German', 'language': 'english'}}

# Defining  paths
INPUT_DIR = "/content/extracted_archive/free-spoken-digit-dataset-master/recordings"  # Replace with the actual path to your audio files
OUTPUT_CSV = r"processed_speaker_data.csv"  # Path to save the processed data

# Run preprocessing
preprocess_audio_files(INPUT_DIR, OUTPUT_CSV)

# Path to the processed data
PROCESSED_CSV = "/content/processed_speaker_data.csv"  # Replace with the actual file path

# Loading the processed CSV file
df = pd.read_csv(PROCESSED_CSV)

# Selecting only numerical columns (MFCC features)
mfcc_columns = [col for col in df.columns if col.startswith('mfcc_')]
df_mfcc = df[mfcc_columns]

# Computing the correlation matrix
correlation_matrix = df_mfcc.corr()

# Displaying the correlation matrix as a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Matrix of MFCC Features')
plt.show()

!cp -r /content/Speaker-Identification-System-MFCC/'Team Project'/data/free-spoken-digit-dataset-master/recordings /content/extracted_archive/free-spoken-digit-dataset-master/

!mkdir /content/extracted_archive/free-spoken-digit-dataset-master

!ls /content/Speaker-Identification-System-MFCC/

# Loading the dataframe.
df = pd.read_csv('processed_speaker_data.csv')

# Defining the mapping for categorical features.
gender_mapping = {'male': 1, 'female': 0}
accent_mapping = {'USA/neutral': 1, 'BEL/French': 2, 'DEU/German': 3, 'GRC/Greek': 4}
language_mapping = {'english': 1}

# Applying the mapping and create new columns
df['gender_encoded'] = df['gender'].map(gender_mapping)
df['accent_encoded'] = df['accent'].map(accent_mapping)
df['language_encoded'] = df['language'].map(language_mapping)

# Dropping the original categorical columns if needed.
df = df.drop(columns=['gender', 'accent', 'language'])

# Print the modified DataFrame to verify changes.
print(df.head())

def extract_mfcc_features(file_path, n_mfcc=13, delta_width=3):
    try:
        audio, sample_rate = librosa.load(file_path, sr=None) # Loading the audio file
        mfcc = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)  # Computing MFCC features

        if mfcc.shape[1] < delta_width: # Ensuring the number of frames is sufficient for delta computation
            print(f"Skipping {file_path}: Too few frames ({mfcc.shape[1]} < {delta_width})")
            return None

        delta_mfcc = librosa.feature.delta(mfcc, width=delta_width) # Computing first derivative of MFCCs
        delta2_mfcc = librosa.feature.delta(mfcc, order=2, width=delta_width) # Computing second derivative of MFCCs
        # Concatenate all features: MFCC + first derivative + second derivative
        combined_mfcc = np.hstack((np.mean(mfcc, axis=1),np.mean(delta_mfcc, axis=1),np.mean(delta2_mfcc, axis=1)))
        return combined_mfcc

    except Exception as e:
        print(f"Error calculating MFCCs for {file_path}: {e}")
        return None

# Function to Save the model, scaler, and label encoder
def save_model(model, scaler, label_encoder, output_path='speaker_identification_model.joblib'):
    joblib.dump({'model': model,'scaler': scaler,'label_encoder': label_encoder}, output_path)
    print(f"Model saved to {output_path}")

# Function to Load the saved model
def load_model(model_path='speaker_identification_model.joblib'):
    saved_data = joblib.load(model_path)
    return saved_data['model'], saved_data['scaler'], saved_data['label_encoder']

# Function to Predict speaker for a new audio file
def predict_speaker(audio_file_path, model_path='speaker_identification_model.joblib'):
    model, scaler, label_encoder = load_model(model_path) # Loading the saved model, scaler, and label encoder
    features = extract_mfcc_features(audio_file_path) # Extracting MFCC features from the new audio file

    if features is None:
        print("Could not extract features from the audio file.")
        return None

    features_scaled = scaler.transform(features.reshape(1, -1))  # Standardizing the features

    # Predicting the speaker
    prediction = model.predict(features_scaled)
    predicted_speaker = label_encoder.inverse_transform(prediction)[0]

    return predicted_speaker

# Loading the dataframe
df = pd.read_csv('/content/processed_speaker_data.csv')

# Defining the target and features
TARGET = "speaker"
FEATURES = [col for col in df.columns if col.startswith("mfcc_")]

# Extracting features and target
X = df[FEATURES]
y = df[TARGET]

# Encoding the target 'speaker' column
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

# Standardizing the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Initializing and training Gradient Boosting Classifier
model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
model.fit(X_train, y_train)

# Evaluating the model
y_pred = model.predict(X_test)
y_pred_decoded = label_encoder.inverse_transform(y_pred)
y_test_decoded = label_encoder.inverse_transform(y_test)

print("Classification Report:")
print(classification_report(y_test_decoded, y_pred_decoded))
print("Accuracy:", accuracy_score(y_test_decoded, y_pred_decoded))

# Saving the model, scaler, and label encoder
save_model(model, scaler, label_encoder)

print("Model saved and ready for prediction!")